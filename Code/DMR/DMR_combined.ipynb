{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37b5a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tomotopy in /Users/oushilin/anaconda3/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<2,>=1.11.0 in /Users/oushilin/anaconda3/lib/python3.11/site-packages (from tomotopy) (1.24.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc1db116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n",
      "\n",
      ">>> Starting analysis for: China_Only\n",
      "PROCESS: Dataset=China_Only | K=10 | Docs=5315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/s5bg7p6n4mgfx_dznzr9hr7c0000gn/T/ipykernel_1483/3018454426.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  model.train(ITER)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: Dataset=China_Only | K=20 | Docs=5315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/s5bg7p6n4mgfx_dznzr9hr7c0000gn/T/ipykernel_1483/3018454426.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  model.train(ITER)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: Dataset=China_Only | K=30 | Docs=5315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/s5bg7p6n4mgfx_dznzr9hr7c0000gn/T/ipykernel_1483/3018454426.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  model.train(ITER)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Starting analysis for: UK_Only\n",
      "PROCESS: Dataset=UK_Only | K=10 | Docs=2117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/s5bg7p6n4mgfx_dznzr9hr7c0000gn/T/ipykernel_1483/3018454426.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  model.train(ITER)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: Dataset=UK_Only | K=20 | Docs=2117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/s5bg7p6n4mgfx_dznzr9hr7c0000gn/T/ipykernel_1483/3018454426.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  model.train(ITER)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: Dataset=UK_Only | K=30 | Docs=2117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/s5bg7p6n4mgfx_dznzr9hr7c0000gn/T/ipykernel_1483/3018454426.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  model.train(ITER)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Starting analysis for: Merged_Global\n",
      "PROCESS: Dataset=Merged_Global | K=10 | Docs=7432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/s5bg7p6n4mgfx_dznzr9hr7c0000gn/T/ipykernel_1483/3018454426.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  model.train(ITER)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: Dataset=Merged_Global | K=20 | Docs=7432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/s5bg7p6n4mgfx_dznzr9hr7c0000gn/T/ipykernel_1483/3018454426.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  model.train(ITER)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESS: Dataset=Merged_Global | K=30 | Docs=7432\n",
      "\n",
      "==================================================\n",
      "ANALYSIS COMPLETE\n",
      "Master Table: narrative_analysis_outputs/master_topic_comparison.csv\n",
      "K-Specific Tables saved in: narrative_analysis_outputs\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jt/s5bg7p6n4mgfx_dznzr9hr7c0000gn/T/ipykernel_1483/3018454426.py:51: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  model.train(ITER)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import tomotopy as tp\n",
    "\n",
    "# --- Configuration ---\n",
    "CHINA_FILE = \"energy_narrative_C_cleaned.csv\"\n",
    "UK_FILE    = \"energy_narrative_W_cleaned.csv\"\n",
    "TEXT_COL   = \"full_content\"\n",
    "SOURCE_COL = \"source\"\n",
    "\n",
    "# Experimental Parameters\n",
    "K_LIST     = [10, 20, 30]  # Number of topics to test per dataset\n",
    "ITER       = 800           # Training iterations\n",
    "SEED       = 42            # For reproducibility\n",
    "OUT_DIR    = \"narrative_analysis_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Basic English Stopwords\n",
    "STOPWORDS = set(\"a an the and or but if while with without of to in on for from as by is are was were be been being this that these those it its they their them we our you your i he she his her at into over under not no do does did doing done can could would should will may might must said says say according also more most one two new just about\".split())\n",
    "\n",
    "# --- Utility Functions ---\n",
    "\n",
    "def tokenize(text: str):\n",
    "    \"\"\"Clean text, remove URLs/non-alpha, and filter stopwords.\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    return [t for t in text.split() if len(t) >= 3 and t not in STOPWORDS]\n",
    "\n",
    "def run_topic_model(df_input, dataset_label, k_val):\n",
    "    \"\"\"\n",
    "    Trains a DMR model for a specific dataset and K value.\n",
    "    Returns a list of dictionaries containing topic metadata.\n",
    "    \"\"\"\n",
    "    print(f\"PROCESS: Dataset={dataset_label} | K={k_val} | Docs={len(df_input)}\")\n",
    "    \n",
    "    # Preprocessing: Tokenize and filter short documents\n",
    "    df_temp = df_input.copy()\n",
    "    df_temp[\"tokens\"] = df_temp[TEXT_COL].apply(tokenize)\n",
    "    df_temp = df_temp[df_temp[\"tokens\"].apply(len) >= 30] \n",
    "    \n",
    "    # Initialize DMR Model\n",
    "    model = tp.DMRModel(k=k_val, seed=SEED)\n",
    "    for row in df_temp.itertuples():\n",
    "        # Metadata allows the model to learn source-specific distributions\n",
    "        meta = f\"source={getattr(row, SOURCE_COL)}\"\n",
    "        model.add_doc(row.tokens, metadata=meta)\n",
    "    \n",
    "    # Training\n",
    "    model.train(ITER)\n",
    "    \n",
    "    # Extract Topic Results\n",
    "    topic_results = []\n",
    "    for k in range(k_val):\n",
    "        # Get top 15 words for each topic\n",
    "        words = \", \".join([w for w, _ in model.get_topic_words(k, top_n=15)])\n",
    "        topic_results.append({\n",
    "            \"Dataset_Type\": dataset_label,\n",
    "            \"Topic_Count_K\": k_val,\n",
    "            \"Topic_ID\": k,\n",
    "            \"Top_Keywords\": words,\n",
    "            \"Log_Likelihood\": model.ll_per_word\n",
    "        })\n",
    "    return topic_results\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "\n",
    "# 1. Load and Tag Data\n",
    "print(\"Loading data files...\")\n",
    "df_c = pd.read_csv(CHINA_FILE)\n",
    "df_w = pd.read_csv(UK_FILE)\n",
    "\n",
    "df_c[\"region\"] = \"China\"\n",
    "df_w[\"region\"] = \"UK\"\n",
    "\n",
    "# Create the merged dataset\n",
    "df_merged = pd.concat([df_c, df_w], ignore_index=True)\n",
    "\n",
    "# Define the three experimental arms\n",
    "tasks = [\n",
    "    (df_c, \"China_Only\"),\n",
    "    (df_w, \"UK_Only\"),\n",
    "    (df_merged, \"Merged_Global\")\n",
    "]\n",
    "\n",
    "master_data = []\n",
    "\n",
    "# 2. Run Nested Loops (Dataset x K-Value)\n",
    "for dataframe, label in tasks:\n",
    "    print(f\"\\n>>> Starting analysis for: {label}\")\n",
    "    for k in K_LIST:\n",
    "        results = run_topic_model(dataframe, label, k)\n",
    "        master_data.extend(results)\n",
    "\n",
    "# 3. Consolidation and Output\n",
    "summary_df = pd.DataFrame(master_data)\n",
    "\n",
    "# Export the Master Comparison Table\n",
    "master_output_path = os.path.join(OUT_DIR, \"master_topic_comparison.csv\")\n",
    "summary_df.to_csv(master_output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Export separate files for each K for easier side-by-side reading\n",
    "for k in K_LIST:\n",
    "    k_subset = summary_df[summary_df[\"Topic_Count_K\"] == k]\n",
    "    k_subset.to_csv(os.path.join(OUT_DIR, f\"comparison_table_K{k}.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(f\"Master Table: {master_output_path}\")\n",
    "print(f\"K-Specific Tables saved in: {OUT_DIR}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
