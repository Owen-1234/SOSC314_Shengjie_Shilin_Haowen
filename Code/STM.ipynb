{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b69d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tomotopy in c:\\users\\baish\\appdata\\roaming\\python\\python311\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<2,>=1.11.0 in c:\\users\\baish\\appdata\\roaming\\python\\python311\\site-packages (from tomotopy) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f90670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows total: 7432\n",
      "Columns: ['url', 'title', 'publish_date', 'source', 'keyword', 'full_content', 'cleaned_content', 'year', 'month', 'day', 'media']\n",
      "Using metadata columns: ['media', 'source', 'keyword']\n",
      "\n",
      "Precomputing tokens + metadata strings (only once)...\n",
      "Docs kept (len>= 30 tokens): 7349 / 7432\n",
      "\n",
      "======================================================================\n",
      "Training DMR (STM-like) model with K=10\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baish\\AppData\\Local\\Temp\\ipykernel_38848\\2203993121.py:150: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  model.train(LOG_EVERY)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=10 | Iter  100/800 | ll_per_word=-8.5352\n",
      "K=10 | Iter  200/800 | ll_per_word=-8.5074\n",
      "K=10 | Iter  300/800 | ll_per_word=-8.4988\n",
      "K=10 | Iter  400/800 | ll_per_word=-8.4925\n",
      "K=10 | Iter  500/800 | ll_per_word=-8.4860\n",
      "K=10 | Iter  600/800 | ll_per_word=-8.4811\n",
      "K=10 | Iter  700/800 | ll_per_word=-8.4773\n",
      "K=10 | Iter  800/800 | ll_per_word=-8.4751\n",
      "Saved top words -> stm_like_outputs_4sources\\K10_top_words.txt\n",
      "Saved topic prevalence -> stm_like_outputs_4sources\\K10_topic_prevalence_by_source.csv\n",
      "Saved group summary -> stm_like_outputs_4sources\\K10_group_top_topics_summary_by_source.txt\n",
      "\n",
      "======================================================================\n",
      "Training DMR (STM-like) model with K=20\n",
      "======================================================================\n",
      "K=20 | Iter  100/800 | ll_per_word=-8.5643\n",
      "K=20 | Iter  200/800 | ll_per_word=-8.5196\n",
      "K=20 | Iter  300/800 | ll_per_word=-8.5034\n",
      "K=20 | Iter  400/800 | ll_per_word=-8.4950\n",
      "K=20 | Iter  500/800 | ll_per_word=-8.4882\n",
      "K=20 | Iter  600/800 | ll_per_word=-8.4823\n",
      "K=20 | Iter  700/800 | ll_per_word=-8.4799\n",
      "K=20 | Iter  800/800 | ll_per_word=-8.4766\n",
      "Saved top words -> stm_like_outputs_4sources\\K20_top_words.txt\n",
      "Saved topic prevalence -> stm_like_outputs_4sources\\K20_topic_prevalence_by_source.csv\n",
      "Saved group summary -> stm_like_outputs_4sources\\K20_group_top_topics_summary_by_source.txt\n",
      "\n",
      "======================================================================\n",
      "Training DMR (STM-like) model with K=30\n",
      "======================================================================\n",
      "K=30 | Iter  100/800 | ll_per_word=-8.5758\n",
      "K=30 | Iter  200/800 | ll_per_word=-8.5189\n",
      "K=30 | Iter  300/800 | ll_per_word=-8.4974\n",
      "K=30 | Iter  400/800 | ll_per_word=-8.4812\n",
      "K=30 | Iter  500/800 | ll_per_word=-8.4711\n",
      "K=30 | Iter  600/800 | ll_per_word=-8.4644\n",
      "K=30 | Iter  700/800 | ll_per_word=-8.4618\n",
      "K=30 | Iter  800/800 | ll_per_word=-8.4583\n",
      "Saved top words -> stm_like_outputs_4sources\\K30_top_words.txt\n",
      "Saved topic prevalence -> stm_like_outputs_4sources\\K30_topic_prevalence_by_source.csv\n",
      "Saved group summary -> stm_like_outputs_4sources\\K30_group_top_topics_summary_by_source.txt\n",
      "\n",
      "All done. Outputs are in: stm_like_outputs_4sources\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import tomotopy as tp\n",
    "\n",
    "china_file = \"energy_narrative_C_cleaned.csv\"\n",
    "uk_file    = \"energy_narrative_W_cleaned.csv\"\n",
    "\n",
    "text_col = \"full_content\"      \n",
    "source_col = \"source\"          \n",
    "\n",
    "optional_meta_cols = [\"keyword\"]  \n",
    "\n",
    "# topic number\n",
    "K_LIST = [10, 20, 30]\n",
    "\n",
    "# training parameters\n",
    "ITER = 800\n",
    "LOG_EVERY = 100\n",
    "SEED = 42\n",
    "\n",
    "# output\n",
    "OUT_DIR = \"stm_like_outputs_4sources\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "#  Read in data+merge\n",
    "df_c = pd.read_csv(china_file)\n",
    "df_w = pd.read_csv(uk_file)\n",
    "\n",
    "# Add a media tag to each file\n",
    "df_c[\"media\"] = \"China\"\n",
    "df_w[\"media\"] = \"UK\"\n",
    "\n",
    "df = pd.concat([df_c, df_w], ignore_index=True)\n",
    "\n",
    "print(\"Loaded rows total:\", len(df))\n",
    "print(\"Columns:\", list(df.columns))\n",
    "\n",
    "# Necessary column inspection\n",
    "for c in [text_col, source_col]:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Column '{c}' not found. Please check your CSV files.\")\n",
    "\n",
    "# Metadata column: Mandatory inclusion of media+source, optional is added\n",
    "meta_cols = [\"media\", source_col]\n",
    "for c in optional_meta_cols:\n",
    "    if c in df.columns:\n",
    "        meta_cols.append(c)\n",
    "\n",
    "print(\"Using metadata columns:\", meta_cols)\n",
    "\n",
    "df = df.dropna(subset=[text_col, source_col]).copy()\n",
    "df[text_col] = df[text_col].astype(str)\n",
    "\n",
    "#  Text preprocessing (one-time, non repetitive)\n",
    "STOPWORDS = set(\"\"\"\n",
    "a an the and or but if while with without of to in on for from as by is are was were be been being\n",
    "this that these those it its they their them we our you your i he she his her at into over under\n",
    "not no do does did doing done can could would should will may might must\n",
    "said says say according also more most one two new just about\n",
    "\"\"\".split())\n",
    "\n",
    "def tokenize(text: str):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)      # romove URL\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)    # keep letters only\n",
    "    toks = [t for t in text.split() if len(t) >= 3 and t not in STOPWORDS]\n",
    "    return toks\n",
    "\n",
    "def build_meta(row):\n",
    "    parts = []\n",
    "    for c in meta_cols:\n",
    "        val = str(row.get(c, \"NA\")).strip()\n",
    "        if val == \"\" or val.lower() == \"nan\":\n",
    "            val = \"NA\"\n",
    "        val = re.sub(r\"\\s+\", \"_\", val)       # avoid space\n",
    "        parts.append(f\"{c}={val}\")\n",
    "    return \" \".join(parts)\n",
    "\n",
    "print(\"\\nPrecomputing tokens + metadata strings (only once)...\")\n",
    "df[\"meta_str\"] = df.apply(build_meta, axis=1)\n",
    "df[\"tokens\"] = df[text_col].apply(tokenize)\n",
    "\n",
    "# Filter short text (reduce noise)\n",
    "MIN_TOKENS = 30\n",
    "df_use = df[df[\"tokens\"].apply(len) >= MIN_TOKENS].copy()\n",
    "print(f\"Docs kept (len>= {MIN_TOKENS} tokens): {len(df_use)} / {len(df)}\")\n",
    "\n",
    "docs_tokens = df_use[\"tokens\"].tolist()\n",
    "docs_meta   = df_use[\"meta_str\"].tolist()\n",
    "\n",
    "# Group by metadata to calculate topic popularity\n",
    "def parse_group(meta_str, key):\n",
    "    m = re.search(rf\"{key}=([^\\s]+)\", meta_str)\n",
    "    return m.group(1) if m else \"NA\"\n",
    "\n",
    "def compute_group_topic_prevalence(model, meta_list, group_key):\n",
    "    \"\"\"\n",
    "    output DataFrame：\n",
    "    group, n_docs, topic_0 ... topic_{K-1}\n",
    "    \"\"\"\n",
    "    K = model.k\n",
    "    group_thetas = {}\n",
    "\n",
    "    for doc, meta_str in zip(model.docs, meta_list):\n",
    "        g = parse_group(meta_str, group_key)\n",
    "        theta = doc.get_topic_dist()\n",
    "        group_thetas.setdefault(g, []).append(theta)\n",
    "\n",
    "    rows = []\n",
    "    for g, thetas in group_thetas.items():\n",
    "        n = len(thetas)\n",
    "        avg = [sum(t[k] for t in thetas) / n for k in range(K)]\n",
    "        row = {\"group\": g, \"n_docs\": n}\n",
    "        for k in range(K):\n",
    "            row[f\"topic_{k}\"] = avg[k]\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"n_docs\", ascending=False)\n",
    "\n",
    "# Output requirement: Compare four newspapers together → Group by source\n",
    "GROUP_KEY = source_col\n",
    "\n",
    "#  run K = 10 / 20 / 30\n",
    "for K in K_LIST:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Training DMR (STM-like) model with K={K}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    model = tp.DMRModel(k=K, alpha=0.1, sigma=1.0, seed=SEED)\n",
    "\n",
    "    # load file\n",
    "    for toks, meta_str in zip(docs_tokens, docs_meta):\n",
    "        model.add_doc(toks, metadata=meta_str)\n",
    "\n",
    "    # train\n",
    "    for i in range(0, ITER, LOG_EVERY):\n",
    "        model.train(LOG_EVERY)\n",
    "        print(f\"K={K} | Iter {i+LOG_EVERY:4d}/{ITER} | ll_per_word={model.ll_per_word:.4f}\")\n",
    "\n",
    "    # ---------- output 1：top words ----------\n",
    "    topn = 12\n",
    "    topwords_path = os.path.join(OUT_DIR, f\"K{K}_top_words.txt\")\n",
    "    with open(topwords_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"DMR (STM-like) model | K={K}\\n\")\n",
    "        f.write(f\"Final ll_per_word: {model.ll_per_word:.6f}\\n\\n\")\n",
    "        for k in range(K):\n",
    "            words = model.get_topic_words(k, top_n=topn)\n",
    "            f.write(f\"Topic {k}:\\n\")\n",
    "            f.write(\", \".join([w for w, _ in words]) + \"\\n\\n\")\n",
    "    print(f\"Saved top words -> {topwords_path}\")\n",
    "\n",
    "    # ---------- output 2：按 source（四报纸）topic prevalence ----------\n",
    "    prev_df = compute_group_topic_prevalence(model, docs_meta, group_key=GROUP_KEY)\n",
    "    prev_path = os.path.join(OUT_DIR, f\"K{K}_topic_prevalence_by_{GROUP_KEY}.csv\")\n",
    "    prev_df.to_csv(prev_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved topic prevalence -> {prev_path}\")\n",
    "\n",
    "    # ---------- output 3: top topics summary ----------\n",
    "    summary_path = os.path.join(OUT_DIR, f\"K{K}_group_top_topics_summary_by_{GROUP_KEY}.txt\")\n",
    "    topic_cols = [f\"topic_{k}\" for k in range(K)]\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Group top topics summary | K={K} | grouped by {GROUP_KEY}\\n\\n\")\n",
    "        for _, row in prev_df.iterrows():\n",
    "            g = row[\"group\"]\n",
    "            n_docs = int(row[\"n_docs\"])\n",
    "            top3 = sorted(topic_cols, key=lambda c: row[c], reverse=True)[:3]\n",
    "            f.write(f\"Group: {g} (n_docs={n_docs})\\n\")\n",
    "            for tc in top3:\n",
    "                k_id = int(tc.split('_')[1])\n",
    "                top_words = \", \".join([w for w, _ in model.get_topic_words(k_id, top_n=6)])\n",
    "                f.write(f\"  {tc}: {row[tc]:.3f} | {top_words}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    print(f\"Saved group summary -> {summary_path}\")\n",
    "\n",
    "print(\"\\nAll done. Outputs are in:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
