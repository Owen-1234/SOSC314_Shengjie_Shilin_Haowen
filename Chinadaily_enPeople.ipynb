{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83747ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from gdeltdoc import GdeltDoc, Filters \n",
    "from newspaper import Article, Config\n",
    "\n",
    "KEYWORDS_LIST = [\"energy transition\", \"carbon neutrality\", \"climate policy\"]\n",
    "DOMAINS = [\"chinadaily.com.cn\", \"en.people.cn\"]\n",
    "START_DATE = \"2020-01-01\"\n",
    "END_DATE = \"2025-12-31\" \n",
    "WINDOW_DAYS = 14\n",
    "\n",
    "desktop_path = Path.home() / \"Desktop\"\n",
    "target_folder = desktop_path / \"SOSC314\" / \"Data\"\n",
    "target_folder.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_FILE = target_folder / \"energy_narrative_2020_2025.csv\"\n",
    "\n",
    "print(f\"System initialized. Data will be stored at: {OUTPUT_FILE}\")\n",
    "\n",
    "def get_metadata_batch(kw, domain, start_dt, end_dt):\n",
    "    gd = GdeltDoc()\n",
    "    f = Filters(keyword=kw, domain=domain, start_date=start_dt, end_date=end_dt)\n",
    "    try:\n",
    "        articles = gd.article_search(f)\n",
    "        return articles\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def scrape_full_text(url):\n",
    "    config = Config()\n",
    "    config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "    config.request_timeout = 15 \n",
    "    try:\n",
    "        article = Article(url, config=config)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        if len(article.text) > 150:\n",
    "            return article.text\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "all_metadata_list = []\n",
    "print(f\"\\n--- Phase 1: Metadata Retrieval ({START_DATE} to {END_DATE}) ---\")\n",
    "\n",
    "for kw in KEYWORDS_LIST:\n",
    "    print(f\"\\nSearching Keyword: [{kw}]\")\n",
    "    curr_start = datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "    final_end = datetime.strptime(END_DATE, '%Y-%m-%d')\n",
    "    \n",
    "    while curr_start < final_end:\n",
    "        curr_end = curr_start + timedelta(days=WINDOW_DAYS)\n",
    "        s_str = curr_start.strftime('%Y-%m-%d')\n",
    "        e_str = curr_end.strftime('%Y-%m-%d')\n",
    "        \n",
    "        print(f\"  Fetching: {s_str} to {e_str} ... \", end=\"\", flush=True)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for dom in DOMAINS:\n",
    "            batch = get_metadata_batch(kw, dom, s_str, e_str)\n",
    "            if not batch.empty:\n",
    "                batch['source_label'] = dom\n",
    "                batch['search_keyword'] = kw\n",
    "                all_metadata_list.append(batch)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Done! ({end_time - start_time:.1f}s)\")\n",
    "        \n",
    "        curr_start = curr_end\n",
    "        time.sleep(1)\n",
    "\n",
    "if not all_metadata_list:\n",
    "    print(\"\\nError: No metadata retrieved. Check connection.\")\n",
    "else:\n",
    "    df_metadata = pd.concat(all_metadata_list).drop_duplicates(subset=['url'])\n",
    "    print(f\"\\nTotal Unique Articles Found: {len(df_metadata)}\")\n",
    "\n",
    "    print(\"\\n--- Phase 2: Content Scraping ---\")\n",
    "    final_results = []\n",
    "\n",
    "    for index, row in tqdm(df_metadata.iterrows(), total=len(df_metadata)):\n",
    "        content = scrape_full_text(row['url'])\n",
    "        if content:\n",
    "            final_results.append({\n",
    "                'url': row['url'],\n",
    "                'title': row.get('title', 'N/A'),\n",
    "                'publish_date': row.get('seendate', 'N/A'),\n",
    "                'source': row['source_label'],\n",
    "                'keyword': row['search_keyword'],\n",
    "                'full_content': content\n",
    "            })\n",
    "        \n",
    "        time.sleep(1.2)\n",
    "        \n",
    "        if len(final_results) > 0 and len(final_results) % 20 == 0:\n",
    "            pd.DataFrame(final_results).to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    if final_results:\n",
    "        df_final = pd.DataFrame(final_results)\n",
    "        df_final.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nSuccess! Total valid records: {len(df_final)}\")\n",
    "        print(f\"Path: {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"\\nProcess finished with 0 valid records.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
